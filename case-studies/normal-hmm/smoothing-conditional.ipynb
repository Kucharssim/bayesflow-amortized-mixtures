{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonkucharsky/projects/bayesflow/amortized-mixture/.venv/lib/python3.11/site-packages/bayesflow/trainers.py:27: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "# for accessing src, stan, etc.\n",
    "sys.path.append(os.path.abspath(os.path.join(\"../..\")))\n",
    "\n",
    "import bayesflow as bf\n",
    "import numpy as np\n",
    "from src.AmortizedMixture import *\n",
    "from src.models.NormalHmm import *\n",
    "\n",
    "from model import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_net = DependentClassificator(num_outputs=model.n_cls, bidirectional=True, num_conv_layers=4)\n",
    "amortizer = AmortizedMixture(inference_net=inference_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialized empty loss history.\n",
      "INFO:root:Initialized networks from scratch.\n",
      "INFO:root:Performing a consistency check with provided components...\n",
      "INFO:root:Done.\n"
     ]
    }
   ],
   "source": [
    "trainer = bf.trainers.Trainer(amortizer=amortizer, generative_model=model, configurator=lambda x: x, checkpoint_path=\"checkpoints/smoothing-conditional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1 µs, total: 2 µs\n",
      "Wall time: 3.1 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1: 100%|██████████| 1000/1000 [03:51<00:00,  4.31it/s, Epoch: 1, Iter: 1000,Loss: 10.120,Avg.Loss: 13.758,LR: 5.00E-04]\n",
      "Training epoch 2: 100%|██████████| 1000/1000 [03:56<00:00,  4.24it/s, Epoch: 2, Iter: 1000,Loss: 9.645,Avg.Loss: 10.168,LR: 5.00E-04]\n",
      "Training epoch 3: 100%|██████████| 1000/1000 [04:03<00:00,  4.10it/s, Epoch: 3, Iter: 1000,Loss: 9.422,Avg.Loss: 9.671,LR: 4.99E-04]\n",
      "Training epoch 4: 100%|██████████| 1000/1000 [04:02<00:00,  4.13it/s, Epoch: 4, Iter: 1000,Loss: 9.746,Avg.Loss: 9.605,LR: 4.98E-04]\n",
      "Training epoch 5: 100%|██████████| 1000/1000 [04:02<00:00,  4.13it/s, Epoch: 5, Iter: 1000,Loss: 10.031,Avg.Loss: 9.471,LR: 4.97E-04]\n",
      "Training epoch 6: 100%|██████████| 1000/1000 [03:59<00:00,  4.17it/s, Epoch: 6, Iter: 1000,Loss: 8.584,Avg.Loss: 9.484,LR: 4.96E-04]\n",
      "Training epoch 7: 100%|██████████| 1000/1000 [04:06<00:00,  4.06it/s, Epoch: 7, Iter: 1000,Loss: 9.520,Avg.Loss: 9.397,LR: 4.94E-04]\n",
      "Training epoch 8: 100%|██████████| 1000/1000 [03:56<00:00,  4.23it/s, Epoch: 8, Iter: 1000,Loss: 8.732,Avg.Loss: 9.324,LR: 4.92E-04]\n",
      "Training epoch 9: 100%|██████████| 1000/1000 [04:05<00:00,  4.07it/s, Epoch: 9, Iter: 1000,Loss: 8.564,Avg.Loss: 9.411,LR: 4.90E-04]\n",
      "Training epoch 10: 100%|██████████| 1000/1000 [04:03<00:00,  4.11it/s, Epoch: 10, Iter: 1000,Loss: 9.219,Avg.Loss: 9.334,LR: 4.88E-04]\n",
      "Training epoch 11: 100%|██████████| 1000/1000 [04:02<00:00,  4.12it/s, Epoch: 11, Iter: 1000,Loss: 10.744,Avg.Loss: 9.273,LR: 4.85E-04]\n",
      "Training epoch 12: 100%|██████████| 1000/1000 [04:08<00:00,  4.02it/s, Epoch: 12, Iter: 1000,Loss: 8.929,Avg.Loss: 9.291,LR: 4.82E-04]\n",
      "Training epoch 13: 100%|██████████| 1000/1000 [04:09<00:00,  4.00it/s, Epoch: 13, Iter: 1000,Loss: 9.533,Avg.Loss: 9.310,LR: 4.79E-04]\n",
      "Training epoch 14: 100%|██████████| 1000/1000 [04:08<00:00,  4.02it/s, Epoch: 14, Iter: 1000,Loss: 9.347,Avg.Loss: 9.298,LR: 4.76E-04]\n",
      "Training epoch 15: 100%|██████████| 1000/1000 [04:09<00:00,  4.01it/s, Epoch: 15, Iter: 1000,Loss: 9.262,Avg.Loss: 9.301,LR: 4.73E-04]\n",
      "Training epoch 16: 100%|██████████| 1000/1000 [03:58<00:00,  4.18it/s, Epoch: 16, Iter: 1000,Loss: 8.820,Avg.Loss: 9.305,LR: 4.69E-04]\n",
      "Training epoch 17: 100%|██████████| 1000/1000 [04:01<00:00,  4.13it/s, Epoch: 17, Iter: 1000,Loss: 10.450,Avg.Loss: 9.283,LR: 4.65E-04]\n",
      "Training epoch 18: 100%|██████████| 1000/1000 [04:01<00:00,  4.14it/s, Epoch: 18, Iter: 1000,Loss: 9.589,Avg.Loss: 9.273,LR: 4.61E-04]\n",
      "Training epoch 19: 100%|██████████| 1000/1000 [03:58<00:00,  4.18it/s, Epoch: 19, Iter: 1000,Loss: 9.958,Avg.Loss: 9.199,LR: 4.57E-04]\n",
      "Training epoch 20: 100%|██████████| 1000/1000 [03:50<00:00,  4.33it/s, Epoch: 20, Iter: 1000,Loss: 10.467,Avg.Loss: 9.299,LR: 4.52E-04]\n",
      "Training epoch 21: 100%|██████████| 1000/1000 [04:01<00:00,  4.15it/s, Epoch: 21, Iter: 1000,Loss: 9.690,Avg.Loss: 9.315,LR: 4.48E-04]\n",
      "Training epoch 22: 100%|██████████| 1000/1000 [04:03<00:00,  4.11it/s, Epoch: 22, Iter: 1000,Loss: 9.129,Avg.Loss: 9.270,LR: 4.43E-04]\n",
      "Training epoch 23: 100%|██████████| 1000/1000 [04:06<00:00,  4.05it/s, Epoch: 23, Iter: 1000,Loss: 10.168,Avg.Loss: 9.263,LR: 4.38E-04]\n",
      "Training epoch 24: 100%|██████████| 1000/1000 [04:10<00:00,  4.00it/s, Epoch: 24, Iter: 1000,Loss: 9.955,Avg.Loss: 9.245,LR: 4.32E-04]\n",
      "Training epoch 25: 100%|██████████| 1000/1000 [03:42<00:00,  4.50it/s, Epoch: 25, Iter: 1000,Loss: 9.321,Avg.Loss: 9.280,LR: 4.27E-04]\n",
      "Training epoch 26: 100%|██████████| 1000/1000 [03:25<00:00,  4.87it/s, Epoch: 26, Iter: 1000,Loss: 11.343,Avg.Loss: 9.355,LR: 4.21E-04]\n",
      "Training epoch 27: 100%|██████████| 1000/1000 [03:25<00:00,  4.86it/s, Epoch: 27, Iter: 1000,Loss: 9.426,Avg.Loss: 9.257,LR: 4.15E-04]\n",
      "Training epoch 28: 100%|██████████| 1000/1000 [03:36<00:00,  4.61it/s, Epoch: 28, Iter: 1000,Loss: 11.967,Avg.Loss: 9.186,LR: 4.09E-04]\n",
      "Training epoch 29: 100%|██████████| 1000/1000 [03:37<00:00,  4.60it/s, Epoch: 29, Iter: 1000,Loss: 8.060,Avg.Loss: 9.275,LR: 4.03E-04]\n",
      "Training epoch 30: 100%|██████████| 1000/1000 [03:36<00:00,  4.62it/s, Epoch: 30, Iter: 1000,Loss: 9.516,Avg.Loss: 9.227,LR: 3.97E-04]\n",
      "Training epoch 31: 100%|██████████| 1000/1000 [03:34<00:00,  4.67it/s, Epoch: 31, Iter: 1000,Loss: 10.434,Avg.Loss: 9.215,LR: 3.91E-04]\n",
      "Training epoch 32: 100%|██████████| 1000/1000 [03:31<00:00,  4.73it/s, Epoch: 32, Iter: 1000,Loss: 9.609,Avg.Loss: 9.252,LR: 3.84E-04]\n",
      "Training epoch 33: 100%|██████████| 1000/1000 [03:28<00:00,  4.79it/s, Epoch: 33, Iter: 1000,Loss: 8.753,Avg.Loss: 9.235,LR: 3.77E-04]\n",
      "Training epoch 34: 100%|██████████| 1000/1000 [03:29<00:00,  4.78it/s, Epoch: 34, Iter: 1000,Loss: 8.349,Avg.Loss: 9.268,LR: 3.70E-04]\n",
      "Training epoch 35: 100%|██████████| 1000/1000 [03:28<00:00,  4.80it/s, Epoch: 35, Iter: 1000,Loss: 10.138,Avg.Loss: 9.223,LR: 3.64E-04]\n",
      "Training epoch 36: 100%|██████████| 1000/1000 [03:29<00:00,  4.78it/s, Epoch: 36, Iter: 1000,Loss: 8.926,Avg.Loss: 9.197,LR: 3.56E-04]\n",
      "Training epoch 37: 100%|██████████| 1000/1000 [03:24<00:00,  4.90it/s, Epoch: 37, Iter: 1000,Loss: 8.978,Avg.Loss: 9.297,LR: 3.49E-04]\n",
      "Training epoch 38: 100%|██████████| 1000/1000 [03:37<00:00,  4.59it/s, Epoch: 38, Iter: 1000,Loss: 9.647,Avg.Loss: 9.232,LR: 3.42E-04]\n",
      "Training epoch 39: 100%|██████████| 1000/1000 [03:44<00:00,  4.46it/s, Epoch: 39, Iter: 1000,Loss: 7.964,Avg.Loss: 9.235,LR: 3.35E-04]\n",
      "Training epoch 40: 100%|██████████| 1000/1000 [03:40<00:00,  4.54it/s, Epoch: 40, Iter: 1000,Loss: 8.756,Avg.Loss: 9.163,LR: 3.27E-04]\n",
      "Training epoch 41: 100%|██████████| 1000/1000 [03:43<00:00,  4.48it/s, Epoch: 41, Iter: 1000,Loss: 8.801,Avg.Loss: 9.207,LR: 3.20E-04]\n",
      "Training epoch 42: 100%|██████████| 1000/1000 [03:47<00:00,  4.40it/s, Epoch: 42, Iter: 1000,Loss: 9.284,Avg.Loss: 9.222,LR: 3.12E-04]\n",
      "Training epoch 43: 100%|██████████| 1000/1000 [03:43<00:00,  4.48it/s, Epoch: 43, Iter: 1000,Loss: 11.285,Avg.Loss: 9.203,LR: 3.05E-04]\n",
      "Training epoch 44: 100%|██████████| 1000/1000 [03:38<00:00,  4.59it/s, Epoch: 44, Iter: 1000,Loss: 8.606,Avg.Loss: 9.264,LR: 2.97E-04]\n",
      "Training epoch 45: 100%|██████████| 1000/1000 [03:38<00:00,  4.58it/s, Epoch: 45, Iter: 1000,Loss: 9.926,Avg.Loss: 9.195,LR: 2.89E-04]\n",
      "Training epoch 46: 100%|██████████| 1000/1000 [03:36<00:00,  4.63it/s, Epoch: 46, Iter: 1000,Loss: 9.555,Avg.Loss: 9.185,LR: 2.81E-04]\n",
      "Training epoch 47: 100%|██████████| 1000/1000 [03:36<00:00,  4.61it/s, Epoch: 47, Iter: 1000,Loss: 8.774,Avg.Loss: 9.272,LR: 2.74E-04]\n",
      "Training epoch 48: 100%|██████████| 1000/1000 [03:33<00:00,  4.69it/s, Epoch: 48, Iter: 1000,Loss: 6.171,Avg.Loss: 9.146,LR: 2.66E-04]\n",
      "Training epoch 49: 100%|██████████| 1000/1000 [03:33<00:00,  4.69it/s, Epoch: 49, Iter: 1000,Loss: 10.365,Avg.Loss: 9.172,LR: 2.58E-04]\n",
      "Training epoch 50: 100%|██████████| 1000/1000 [03:25<00:00,  4.88it/s, Epoch: 50, Iter: 1000,Loss: 8.599,Avg.Loss: 9.191,LR: 2.50E-04]\n",
      "Training epoch 51: 100%|██████████| 1000/1000 [03:17<00:00,  5.07it/s, Epoch: 51, Iter: 1000,Loss: 10.591,Avg.Loss: 9.178,LR: 2.42E-04]\n",
      "Training epoch 52: 100%|██████████| 1000/1000 [03:21<00:00,  4.97it/s, Epoch: 52, Iter: 1000,Loss: 8.682,Avg.Loss: 9.203,LR: 2.34E-04]\n",
      "Training epoch 53: 100%|██████████| 1000/1000 [03:15<00:00,  5.11it/s, Epoch: 53, Iter: 1000,Loss: 10.122,Avg.Loss: 9.208,LR: 2.26E-04]\n",
      "Training epoch 54: 100%|██████████| 1000/1000 [03:16<00:00,  5.10it/s, Epoch: 54, Iter: 1000,Loss: 10.040,Avg.Loss: 9.173,LR: 2.19E-04]\n",
      "Training epoch 55: 100%|██████████| 1000/1000 [03:19<00:00,  5.00it/s, Epoch: 55, Iter: 1000,Loss: 8.535,Avg.Loss: 9.209,LR: 2.11E-04]\n",
      "Training epoch 56: 100%|██████████| 1000/1000 [02:52<00:00,  5.79it/s, Epoch: 56, Iter: 1000,Loss: 9.969,Avg.Loss: 9.186,LR: 2.03E-04]\n",
      "Training epoch 57: 100%|██████████| 1000/1000 [03:07<00:00,  5.32it/s, Epoch: 57, Iter: 1000,Loss: 8.379,Avg.Loss: 9.196,LR: 1.95E-04]\n",
      "Training epoch 58: 100%|██████████| 1000/1000 [03:00<00:00,  5.53it/s, Epoch: 58, Iter: 1000,Loss: 9.889,Avg.Loss: 9.226,LR: 1.88E-04]\n",
      "Training epoch 59: 100%|██████████| 1000/1000 [03:01<00:00,  5.51it/s, Epoch: 59, Iter: 1000,Loss: 7.897,Avg.Loss: 9.206,LR: 1.80E-04]\n",
      "Training epoch 60: 100%|██████████| 1000/1000 [02:46<00:00,  6.00it/s, Epoch: 60, Iter: 1000,Loss: 8.339,Avg.Loss: 9.208,LR: 1.73E-04]\n",
      "Training epoch 61: 100%|██████████| 1000/1000 [02:49<00:00,  5.91it/s, Epoch: 61, Iter: 1000,Loss: 7.739,Avg.Loss: 9.195,LR: 1.65E-04]\n",
      "Training epoch 62: 100%|██████████| 1000/1000 [02:52<00:00,  5.79it/s, Epoch: 62, Iter: 1000,Loss: 10.232,Avg.Loss: 9.175,LR: 1.58E-04]\n",
      "Training epoch 63: 100%|██████████| 1000/1000 [02:49<00:00,  5.90it/s, Epoch: 63, Iter: 1000,Loss: 8.792,Avg.Loss: 9.215,LR: 1.51E-04]\n",
      "Training epoch 64: 100%|██████████| 1000/1000 [02:51<00:00,  5.81it/s, Epoch: 64, Iter: 1000,Loss: 10.353,Avg.Loss: 9.168,LR: 1.44E-04]\n",
      "Training epoch 65: 100%|██████████| 1000/1000 [02:48<00:00,  5.94it/s, Epoch: 65, Iter: 1000,Loss: 8.805,Avg.Loss: 9.183,LR: 1.37E-04]\n",
      "Training epoch 66: 100%|██████████| 1000/1000 [02:49<00:00,  5.90it/s, Epoch: 66, Iter: 1000,Loss: 8.693,Avg.Loss: 9.161,LR: 1.30E-04]\n",
      "Training epoch 67: 100%|██████████| 1000/1000 [02:49<00:00,  5.91it/s, Epoch: 67, Iter: 1000,Loss: 8.378,Avg.Loss: 9.203,LR: 1.23E-04]\n",
      "Training epoch 68: 100%|██████████| 1000/1000 [02:48<00:00,  5.92it/s, Epoch: 68, Iter: 1000,Loss: 8.640,Avg.Loss: 9.208,LR: 1.16E-04]\n",
      "Training epoch 69: 100%|██████████| 1000/1000 [02:47<00:00,  5.95it/s, Epoch: 69, Iter: 1000,Loss: 10.472,Avg.Loss: 9.188,LR: 1.09E-04]\n",
      "Training epoch 70: 100%|██████████| 1000/1000 [02:53<00:00,  5.77it/s, Epoch: 70, Iter: 1000,Loss: 9.578,Avg.Loss: 9.175,LR: 1.03E-04]\n",
      "Training epoch 71: 100%|██████████| 1000/1000 [02:55<00:00,  5.70it/s, Epoch: 71, Iter: 1000,Loss: 8.748,Avg.Loss: 9.170,LR: 9.68E-05]\n",
      "Training epoch 72: 100%|██████████| 1000/1000 [02:57<00:00,  5.63it/s, Epoch: 72, Iter: 1000,Loss: 8.827,Avg.Loss: 9.237,LR: 9.07E-05]\n",
      "Training epoch 73: 100%|██████████| 1000/1000 [02:58<00:00,  5.60it/s, Epoch: 73, Iter: 1000,Loss: 10.944,Avg.Loss: 9.141,LR: 8.47E-05]\n",
      "Training epoch 74: 100%|██████████| 1000/1000 [02:54<00:00,  5.74it/s, Epoch: 74, Iter: 1000,Loss: 8.258,Avg.Loss: 9.232,LR: 7.89E-05]\n",
      "Training epoch 75: 100%|██████████| 1000/1000 [02:55<00:00,  5.69it/s, Epoch: 75, Iter: 1000,Loss: 7.980,Avg.Loss: 9.167,LR: 7.32E-05]\n",
      "Training epoch 76: 100%|██████████| 1000/1000 [02:57<00:00,  5.64it/s, Epoch: 76, Iter: 1000,Loss: 9.347,Avg.Loss: 9.228,LR: 6.78E-05]\n",
      "Training epoch 77: 100%|██████████| 1000/1000 [02:57<00:00,  5.63it/s, Epoch: 77, Iter: 1000,Loss: 11.089,Avg.Loss: 9.184,LR: 6.25E-05]\n",
      "Training epoch 78: 100%|██████████| 1000/1000 [02:57<00:00,  5.64it/s, Epoch: 78, Iter: 1000,Loss: 9.607,Avg.Loss: 9.214,LR: 5.74E-05]\n",
      "Training epoch 79: 100%|██████████| 1000/1000 [02:46<00:00,  6.00it/s, Epoch: 79, Iter: 1000,Loss: 8.622,Avg.Loss: 9.234,LR: 5.25E-05]\n",
      "Training epoch 80: 100%|██████████| 1000/1000 [02:52<00:00,  5.81it/s, Epoch: 80, Iter: 1000,Loss: 10.421,Avg.Loss: 9.137,LR: 4.78E-05]\n",
      "Training epoch 81: 100%|██████████| 1000/1000 [02:54<00:00,  5.73it/s, Epoch: 81, Iter: 1000,Loss: 7.282,Avg.Loss: 9.163,LR: 4.32E-05]\n",
      "Training epoch 82: 100%|██████████| 1000/1000 [02:54<00:00,  5.73it/s, Epoch: 82, Iter: 1000,Loss: 11.091,Avg.Loss: 9.182,LR: 3.89E-05]\n",
      "Training epoch 83: 100%|██████████| 1000/1000 [02:50<00:00,  5.86it/s, Epoch: 83, Iter: 1000,Loss: 8.630,Avg.Loss: 9.163,LR: 3.48E-05]\n",
      "Training epoch 84: 100%|██████████| 1000/1000 [10:25<00:00,  1.60it/s, Epoch: 84, Iter: 1000,Loss: 11.469,Avg.Loss: 9.185,LR: 3.09E-05] \n",
      "Training epoch 85: 100%|██████████| 1000/1000 [52:10<00:00,  3.13s/it, Epoch: 85, Iter: 1000,Loss: 8.238,Avg.Loss: 9.158,LR: 2.73E-05]   \n",
      "Training epoch 86: 100%|██████████| 1000/1000 [02:45<00:00,  6.03it/s, Epoch: 86, Iter: 1000,Loss: 9.072,Avg.Loss: 9.151,LR: 2.38E-05]\n",
      "Training epoch 87: 100%|██████████| 1000/1000 [02:50<00:00,  5.88it/s, Epoch: 87, Iter: 1000,Loss: 8.283,Avg.Loss: 9.216,LR: 2.06E-05]\n",
      "Training epoch 88: 100%|██████████| 1000/1000 [02:48<00:00,  5.94it/s, Epoch: 88, Iter: 1000,Loss: 8.494,Avg.Loss: 9.096,LR: 1.76E-05]\n",
      "Training epoch 89: 100%|██████████| 1000/1000 [02:45<00:00,  6.04it/s, Epoch: 89, Iter: 1000,Loss: 8.616,Avg.Loss: 9.135,LR: 1.48E-05]\n",
      "Training epoch 90: 100%|██████████| 1000/1000 [02:50<00:00,  5.87it/s, Epoch: 90, Iter: 1000,Loss: 9.312,Avg.Loss: 9.182,LR: 1.22E-05]\n",
      "Training epoch 91: 100%|██████████| 1000/1000 [02:49<00:00,  5.90it/s, Epoch: 91, Iter: 1000,Loss: 8.040,Avg.Loss: 9.155,LR: 9.93E-06]\n",
      "Training epoch 92: 100%|██████████| 1000/1000 [02:47<00:00,  5.96it/s, Epoch: 92, Iter: 1000,Loss: 8.506,Avg.Loss: 9.166,LR: 7.86E-06]\n",
      "Training epoch 93: 100%|██████████| 1000/1000 [02:47<00:00,  5.97it/s, Epoch: 93, Iter: 1000,Loss: 8.656,Avg.Loss: 9.140,LR: 6.02E-06]\n",
      "Training epoch 94: 100%|██████████| 1000/1000 [02:47<00:00,  5.96it/s, Epoch: 94, Iter: 1000,Loss: 8.864,Avg.Loss: 9.185,LR: 4.43E-06]\n",
      "Training epoch 95: 100%|██████████| 1000/1000 [02:49<00:00,  5.89it/s, Epoch: 95, Iter: 1000,Loss: 7.035,Avg.Loss: 9.187,LR: 3.08E-06]\n",
      "Training epoch 96: 100%|██████████| 1000/1000 [02:49<00:00,  5.90it/s, Epoch: 96, Iter: 1000,Loss: 9.984,Avg.Loss: 9.140,LR: 1.97E-06]\n",
      "Training epoch 97: 100%|██████████| 1000/1000 [02:49<00:00,  5.91it/s, Epoch: 97, Iter: 1000,Loss: 9.583,Avg.Loss: 9.211,LR: 1.11E-06]\n",
      "Training epoch 98: 100%|██████████| 1000/1000 [02:47<00:00,  5.96it/s, Epoch: 98, Iter: 1000,Loss: 9.519,Avg.Loss: 9.133,LR: 4.94E-07]\n",
      "Training epoch 99: 100%|██████████| 1000/1000 [02:53<00:00,  5.75it/s, Epoch: 99, Iter: 1000,Loss: 9.350,Avg.Loss: 9.187,LR: 1.24E-07]\n",
      "Training epoch 100: 100%|██████████| 1000/1000 [02:53<00:00,  5.78it/s, Epoch: 100, Iter: 1000,Loss: 8.801,Avg.Loss: 9.150,LR: 0.00E+00]\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "h = trainer.train_online(epochs=100, iterations_per_epoch=1000, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
